{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat all the steps for processing on test dataset as on train and doing predictions on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from fancyimpute import SoftImpute\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "INPUT_DIR = '/Users/himani/workspace/assignment/pervazive/expedia/input'\n",
    "MODELS_DIR = '/Users/himani/workspace/assignment/pervazive/expedia/models'\n",
    "OUTPUT_DIR = '/Users/himani/workspace/assignment/pervazive/expedia/output'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = os.path.join(INPUT_DIR, 'train_clean1.csv')\n",
    "test_file = os.path.join(INPUT_DIR, 'test_clean.csv')\n",
    "destinations_file = os.path.join(INPUT_DIR, 'destinations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent duration of stay in test is 1.0\n",
      "[SoftImpute] Max Singular Value of X_init = 51671740.464037\n",
      "[SoftImpute] Iter 1: observed MAE=497.931547 rank=2\n",
      "[SoftImpute] Iter 2: observed MAE=501.394849 rank=2\n",
      "[SoftImpute] Iter 3: observed MAE=504.073970 rank=2\n",
      "[SoftImpute] Iter 4: observed MAE=506.094780 rank=2\n",
      "[SoftImpute] Iter 5: observed MAE=507.594727 rank=2\n",
      "[SoftImpute] Iter 6: observed MAE=508.699851 rank=2\n",
      "[SoftImpute] Iter 7: observed MAE=509.515133 rank=2\n",
      "[SoftImpute] Iter 8: observed MAE=510.120486 rank=2\n",
      "[SoftImpute] Iter 9: observed MAE=510.572392 rank=2\n",
      "[SoftImpute] Iter 10: observed MAE=510.911707 rank=2\n",
      "[SoftImpute] Iter 11: observed MAE=511.167107 rank=2\n",
      "[SoftImpute] Iter 12: observed MAE=511.360370 rank=2\n",
      "[SoftImpute] Iter 13: observed MAE=511.507241 rank=2\n",
      "[SoftImpute] Iter 14: observed MAE=511.619003 rank=2\n",
      "[SoftImpute] Iter 15: observed MAE=511.704230 rank=2\n",
      "[SoftImpute] Iter 16: observed MAE=511.769349 rank=2\n",
      "[SoftImpute] Iter 17: observed MAE=511.819165 rank=2\n",
      "[SoftImpute] Iter 18: observed MAE=511.857352 rank=2\n",
      "[SoftImpute] Iter 19: observed MAE=511.886604 rank=2\n",
      "[SoftImpute] Iter 20: observed MAE=511.909023 rank=2\n",
      "[SoftImpute] Iter 21: observed MAE=511.926214 rank=2\n",
      "[SoftImpute] Iter 22: observed MAE=511.939400 rank=2\n",
      "[SoftImpute] Stopped after iteration 22 for lambda=1033434.809281\n"
     ]
    }
   ],
   "source": [
    "# Now reading in test data and applying similar functions as on train\n",
    "\n",
    "test = pd.read_csv(test_file, parse_dates = ['date_time','srch_ci','srch_co'])\n",
    "\n",
    "# converting srch_ci and srch_co to datetime \n",
    "test['srch_ci'] = pd.to_datetime(test['srch_ci'],format='%Y-%m-%d', errors=\"coerce\")\n",
    "test['srch_co'] = pd.to_datetime(test['srch_co'],format='%Y-%m-%d', errors=\"coerce\")\n",
    "\n",
    "# calculate duration of stay (in days) and creating month\n",
    "\n",
    "test['duration'] = (test['srch_co'] - test['srch_ci']).dt.days\n",
    "print(\"Most frequent duration of stay in test is\",test['duration'].mode()[0])\n",
    "\n",
    "# month\n",
    "test['month'] = test['srch_ci'].dt.month\n",
    "\n",
    "\n",
    "# now deleting date_time, srch_co and srch_ci \n",
    "\n",
    "test.drop(['srch_co','srch_ci'], inplace = True, axis =1)\n",
    "\n",
    "\n",
    "test['duration'].fillna(test['duration'].mode()[0], inplace = True)\n",
    "test['month'].fillna(test['date_time'].dt.month, inplace = True)\n",
    "test['duration'] = test['duration'].astype(int)\n",
    "test['month'] = test['month'].astype(int)\n",
    "\n",
    "\n",
    "df2 = test.loc[:,['user_location_city','hotel_market','orig_destination_distance']]\n",
    "df2_matrix = df2.as_matrix()\n",
    "\n",
    "\n",
    "from fancyimpute import SoftImpute\n",
    "df2_matrix_filled = SoftImpute().complete(df2_matrix)\n",
    "\n",
    "test['user_location_city'] = pd.Series(df2_matrix_filled[:,0])\n",
    "test['hotel_market'] = pd.Series(df2_matrix_filled[:,1])\n",
    "test['orig_destination_distance'] = pd.Series(df2_matrix_filled[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and cols in destination file - (62106, 150)\n"
     ]
    }
   ],
   "source": [
    "destination = pd.read_csv(destinations_file)\n",
    "print(\"Number of rows and cols in destination file -\",destination.shape)\n",
    "\n",
    "pca = PCA(n_components=149)\n",
    "pca.fit_transform(destination[[\"d{0}\".format(i + 1) for i in range(149)]])\n",
    "\n",
    "# creating 20 features from destination file\n",
    "p = PCA(n_components=20, random_state = 3)\n",
    "df = p.fit_transform(destination[[\"d{0}\".format(i + 1) for i in range(20)]])\n",
    "df = pd.DataFrame(df)\n",
    "df[\"srch_destination_id\"] = destination[\"srch_destination_id\"]\n",
    "col = df.columns.tolist()\n",
    "\n",
    "# append pca features to train & test data on srch_destination_id using left join\n",
    "new_test = pd.merge(test, df, on = 'srch_destination_id', how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replacing missing values of destination file\n",
    "# this will be improved by using collaborative filtering to find similar destinations\n",
    "\n",
    "for i in col[:-1]:\n",
    "    new_test[i].fillna(new_test[i].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving final test file \n",
    "\n",
    "new_test.to_csv(os.path.join(INPUT_DIR, 'test_final.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predictions on Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading saved random forests model \n",
    "\n",
    "rfc = joblib.load(rf_models_file)\n",
    "\n",
    "# loading saved xgboost model\n",
    "\n",
    "clf = joblib.load(xg_models_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicting top clusters\n",
    "# same function can be used to predict random forests and xgboosts top clusters\n",
    "\n",
    "def predict():\n",
    "    print(\"Loading model\")\n",
    "    clf = joblib.load(os.path.join(MODELS_DIR, 'model4.pkl'))\n",
    "    print(\"Model loaded\")\n",
    "    print(\"Loading test data\")\n",
    "    new_test = pd.read_csv(os.path.join(INPUT_DIR, 'test_final.csv'), nrows=NROWS)\n",
    "    print(\"Test data loaded\")\n",
    "    test_predictors = [c for c in new_test.columns if c not in ['id','user_id','date_time']]\n",
    "    new_test = new_test[test_predictors]\n",
    "\n",
    "    print(\"Begin prediction\")\n",
    "    n = new_test.shape[0]\n",
    "    top_5 = []\n",
    "    for i in range(0, n, 10000):\n",
    "        end = i + 10000 if i + 10000 < n else n\n",
    "        t = new_test.iloc[i:end]\n",
    "        predicted_prob = clf.predict_proba(t)\n",
    "        for lista in predicted_prob:\n",
    "            top_5.append(' '.join([str(x) for x in lista.argsort()[-5:].tolist()]))\n",
    "        print(\"Completed {} rows\".format(end))\n",
    "\n",
    "    xgb_5recos_df = pd.DataFrame(top_5, columns=['hotel_cluster'])\n",
    "    xgb_5recos_df.to_csv(os.path.join(OUTPUT_DIR, 'xgb_5recos.csv'), index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
